import boto3
import sagemaker
import datetime as dt
import pandas as pd


### Submit the pipeline to SageMaker and start execution


region = boto3.Session().region_name
role = sagemaker.get_execution_role()
sagemaker_session = sagemaker.Session()
sklearn_processor_version="0.23-1"
model_package_group_name="ChurnModelPackageGroup"
pipeline_name= "ChurnModelSMPipeline"
clarify_image = sagemaker.image_uris.retrieve(framework='sklearn',version=sklearn_processor_version,region=region)


def preprocess_data(file_path):
    df = pd.read_csv(file_path)
    ## Convert to datetime columns
    df["firstorder"]=pd.to_datetime(df["firstorder"],errors='coerce')
    df["lastorder"] = pd.to_datetime(df["lastorder"],errors='coerce')
    ## Drop Rows with null values
    df = df.dropna()
    ## Create Column which gives the days between the last order and the first order
    df["first_last_days_diff"] = (df['lastorder']-df['firstorder']).dt.days
    ## Create Column which gives the days between when the customer record was created and the first order
    df['created'] = pd.to_datetime(df['created'])
    df['created_first_days_diff']=(df['created']-df['firstorder']).dt.days
    ## Drop Columns
    df.drop(['custid','created','firstorder','lastorder'],axis=1,inplace=True)
    ## Apply one hot encoding on favday and city columns
    df = pd.get_dummies(df,prefix=['favday','city'],columns=['favday','city'])
    return df


default_bucket_data = 'sagemaker-studio-891376975801-3j1752jty8h'
baseline_data = preprocess_data(f"s3://{default_bucket_data}/data/storedata_total.csv")
baseline_data.pop("retained")
baseline_sample = baseline_data.sample(frac=0.0002)


pd.DataFrame(baseline_sample).to_csv(f"s3://{default_bucket_data}/data/baseline.csv",header=False,index=False)


batch_data = preprocess_data(f"s3://{default_bucket_data}/data/storedata_total.csv")
batch_data.pop("retained")
batch_sample = batch_data.sample(frac=0.2)


pd.DataFrame(batch_sample).to_csv(f"s3://{default_bucket_data}/data/batch.csv",header=False,index=False)


s3_client = boto3.resource('s3')
s3_client.Bucket(default_bucket)
# s3_client.Bucket(default_bucket).upload_file(f"s3://{default_bucket_data}/data/storedata_total.csv",f"s3://{default_bucket}/data/storedata_total.csv")
# s3_client.Bucket(default_bucket).upload_file(f"s3://{default_bucket_data}/data/batch.csv",f"s3://{default_bucket}/data/batch.csv")
# s3_client.Bucket(default_bucket).upload_file(f"s3://{default_bucket_data}/data/baseline.csv",f"s3://{default_bucket}/data/baseline.csv")


s3_client.Bucket(default_bucket).upload_file("churn-modelling/pipelines/customerchurn/preprocess.py","input/code/preprocess.py")


s3_client.Bucket(default_bucket).upload_file("churn-modelling/pipelines/customerchurn/evaluate.py","input/code/evaluate.py")
s3_client.Bucket(default_bucket).upload_file("churn-modelling/pipelines/customerchurn/generate_config.py","input/code/generate_config.py")


get_ipython().getoutput("pip install pipelines")


from churn_modelling.pipelines.customerchurn.pipeline import get_pipeline

pipeline = get_pipeline(
    region = region,
    role=role,
    default_bucket=default_bucket_data,
    model_package_group_name=model_package_group_name,
    pipeline_name=pipeline_name,
    custom_image_uri=clarify_image,
    sklearn_processor_version=sklearn_processor_version
)


pipeline.definition()


pipeline.upsert(role_arn=role)


execution = pipeline.start()


execution.describe()



